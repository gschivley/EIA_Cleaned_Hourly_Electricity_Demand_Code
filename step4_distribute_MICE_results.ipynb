{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Which Input Files to Use\n",
    "The default settings will use the input files recently produced in Step 1) using the notebook `get_eia_demand_data.ipynb`. For those interested in reproducing the exact results included in the repository, you will need to point to the files containing the original `raw` EIA demand data that we querried on 10 Sept 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_with_step1_files = False # used to run step 2 on the most recent files\n",
    "merge_with_10sept2019_files = True # used to reproduce the documented results\n",
    "assert((merge_with_step1_files != merge_with_10sept2019_files) and \n",
    "       (merge_with_step1_files == True or merge_with_10sept2019_files == True)), \"One of these must be true: 'merge_with_step1_files' and 'merge_with_10sept2019_files'\"\n",
    "\n",
    "if merge_with_step1_files:\n",
    "    input_path = './data'\n",
    "\n",
    "if merge_with_10sept2019_files:\n",
    "    # input_path is the path to the downloaded data from Zenodo: https://zenodo.org/record/3517197\n",
    "    input_path = '/BASE/PATH/TO/ZENODO'\n",
    "    input_path += '/data/release_2019_Oct/original_eia_files'\n",
    "    assert(os.path.exists(input_path)), f\"You must set the base directory for the Zenodo data {input_path} does not exist\"\n",
    "    # If you did not run step 1, make the /data directory\n",
    "    if not os.path.exists('./data'):\n",
    "        os.mkdir('./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make output directories\n",
    "out_base = './data/final_results'\n",
    "if not os.path.exists(out_base):\n",
    "    os.mkdir(out_base)\n",
    "    for subdir in ['balancing_authorities', 'regions', 'interconnects', 'contiguous_US']:\n",
    "        os.mkdir(f\"{out_base}/{subdir}\")\n",
    "        print(f\"Final results files will be located here: {out_base}/{subdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 56 balancing authorities that have demand (BA)\n",
    "def return_all_regions():\n",
    "    return [\n",
    "                'AEC', 'AECI', 'CPLE', 'CPLW',\n",
    "                'DUK', 'FMPP', 'FPC',\n",
    "                'FPL', 'GVL', 'HST', 'ISNE',\n",
    "                'JEA', 'LGEE', 'MISO', 'NSB',\n",
    "                'NYIS', 'PJM', 'SC',\n",
    "                'SCEG', 'SOCO',\n",
    "                'SPA', 'SWPP', 'TAL', 'TEC',\n",
    "                'TVA', 'ERCO',\n",
    "                'AVA', 'AZPS', 'BANC', 'BPAT',\n",
    "                'CHPD', 'CISO', 'DOPD',\n",
    "                'EPE', 'GCPD', 'IID',\n",
    "                'IPCO', 'LDWP', 'NEVP', 'NWMT',\n",
    "                'PACE', 'PACW', 'PGE', 'PNM',\n",
    "                'PSCO', 'PSEI', 'SCL', 'SRP',\n",
    "                'TEPC', 'TIDC', 'TPWR', 'WACM',\n",
    "                'WALC', 'WAUW',\n",
    "                'OVEC', 'SEC',\n",
    "                ]\n",
    "\n",
    "# All 54 \"usable\" balancing authorities (BA) (excludes OVEC and SEC)\n",
    "# These 2 have significant\n",
    "# enough reporting problems that we do not impute cleaned data for them.\n",
    "def return_usable_BAs():\n",
    "    return [\n",
    "                'AEC', 'AECI', 'CPLE', 'CPLW',\n",
    "                'DUK', 'FMPP', 'FPC',\n",
    "                'FPL', 'GVL', 'HST', 'ISNE',\n",
    "                'JEA', 'LGEE', 'MISO', 'NSB',\n",
    "                'NYIS', 'PJM', 'SC',\n",
    "                'SCEG', 'SOCO',\n",
    "                'SPA', 'SWPP', 'TAL', 'TEC',\n",
    "                'TVA', 'ERCO',\n",
    "                'AVA', 'AZPS', 'BANC', 'BPAT',\n",
    "                'CHPD', 'CISO', 'DOPD',\n",
    "                'EPE', 'GCPD', 'IID',\n",
    "                'IPCO', 'LDWP', 'NEVP', 'NWMT',\n",
    "                'PACE', 'PACW', 'PGE', 'PNM',\n",
    "                'PSCO', 'PSEI', 'SCL', 'SRP',\n",
    "                'TEPC', 'TIDC', 'TPWR', 'WACM',\n",
    "                'WALC', 'WAUW',\n",
    "                # 'OVEC', 'SEC',\n",
    "                ]\n",
    "\n",
    "# mapping of each balancing authority (BA) to its associated\n",
    "# U.S. interconnect (IC).\n",
    "def return_ICs_from_BAs():\n",
    "    return {\n",
    "        'EASTERN_IC' : [\n",
    "                'AEC', 'AECI', 'CPLE', 'CPLW',\n",
    "                'DUK', 'FMPP', 'FPC',\n",
    "                'FPL', 'GVL', 'HST', 'ISNE',\n",
    "                'JEA', 'LGEE', 'MISO', 'NSB',\n",
    "                'NYIS', 'PJM', 'SC',\n",
    "                'SCEG', 'SOCO',\n",
    "                'SPA', 'SWPP', 'TAL', 'TEC',\n",
    "                'TVA',\n",
    "                'OVEC', 'SEC',\n",
    "                ],\n",
    "        'TEXAS_IC' : [\n",
    "                'ERCO',\n",
    "                ],\n",
    "        'WESTERN_IC' : [\n",
    "                'AVA', 'AZPS', 'BANC', 'BPAT',\n",
    "                'CHPD', 'CISO', 'DOPD',\n",
    "                'EPE', 'GCPD',\n",
    "                'IID',\n",
    "                'IPCO', 'LDWP', 'NEVP', 'NWMT',\n",
    "                'PACE', 'PACW', 'PGE', 'PNM',\n",
    "                'PSCO', 'PSEI', 'SCL', 'SRP',\n",
    "                'TEPC', 'TIDC', 'TPWR', 'WACM',\n",
    "                'WALC', 'WAUW',\n",
    "                ]\n",
    "        }\n",
    "\n",
    "# Defines a mapping between the balancing authorities (BAs)\n",
    "# and their locally defined region based on EIA naming.\n",
    "# This uses a json file defining the mapping.\n",
    "def return_BAs_per_region_map():\n",
    "\n",
    "    regions = {\n",
    "            'CENT' : 'Central',\n",
    "            'MIDW' : 'Midwest',\n",
    "            'TEN' : 'Tennessee',\n",
    "            'SE' : 'Southeast',\n",
    "            'FLA' : 'Florida',\n",
    "            'CAR' : 'Carolinas',\n",
    "            'MIDA' : 'Mid-Atlantic',\n",
    "            'NY' : 'New York',\n",
    "            'NE' : 'New England',\n",
    "            'TEX' : 'Texas',\n",
    "            'CAL' : 'California',\n",
    "            'NW' : 'Northwest',\n",
    "            'SW' : 'Southwest'\n",
    "    }\n",
    "\n",
    "    rtn_map = {}\n",
    "    for k, v in regions.items():\n",
    "        rtn_map[k] = []\n",
    "\n",
    "    # Load EIA's Blancing Authority Acronym table\n",
    "    # https://www.eia.gov/realtime_grid/\n",
    "    df = pd.read_csv('data/balancing_authority_acronyms.csv',\n",
    "            skiprows=1) # skip first row as it is source info\n",
    "\n",
    "    # Loop over all rows and fill map\n",
    "    for idx in df.index:\n",
    "\n",
    "        # Skip Canada and Mexico\n",
    "        if df.loc[idx, 'Region'] in ['Canada', 'Mexico']:\n",
    "            continue\n",
    "\n",
    "        reg_acronym = ''\n",
    "        # Get region to acronym\n",
    "        for k, v in regions.items():\n",
    "            if v == df.loc[idx, 'Region']:\n",
    "                reg_acronym = k\n",
    "                break\n",
    "        assert(reg_acronym != '')\n",
    "\n",
    "        rtn_map[reg_acronym].append(df.loc[idx, 'Code'])\n",
    "\n",
    "    tot = 0\n",
    "    for k, v in rtn_map.items():\n",
    "        tot += len(v)\n",
    "    print(f\"Total US48 BAs mapped {tot}.  Recall 11 are generation only.\")\n",
    "\n",
    "    return rtn_map\n",
    "\n",
    "\n",
    "# Assume the MICE results file is a subset of the original hours\n",
    "def trim_rows_to_match_length(mice, df):\n",
    "    mice_start = mice.loc[0, 'date_time']\n",
    "    mice_end = mice.loc[len(mice.index)-1, 'date_time']\n",
    "    to_drop = []\n",
    "    for idx in df.index:\n",
    "        if df.loc[idx, 'date_time'] != mice_start:\n",
    "            to_drop.append(idx)\n",
    "        else: # stop once equal\n",
    "            break\n",
    "    for idx in reversed(df.index):\n",
    "        if df.loc[idx, 'date_time'] != mice_end:\n",
    "            to_drop.append(idx)\n",
    "        else: # stop once equal\n",
    "            break\n",
    "    \n",
    "    df = df.drop(to_drop, axis=0)\n",
    "    df = df.reset_index()\n",
    "    assert(len(mice.index) == len(df.index))\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load balancing authority files already containing the full MICE results.\n",
    "# Aggregate associated regions into regional, interconnect, or CONUS files.\n",
    "# Treat 'MISSING' and 'EMPTY' values as zeros when aggregating.\n",
    "def merge_BAs(region, bas, out_base, folder):\n",
    "    \n",
    "    print(region, bas)\n",
    "    \n",
    "    # Remove BAs which are generation only as well as SEC and OVEC.\n",
    "    # See main README regarding SEC and OVEC.\n",
    "    usable_BAs = return_usable_BAs()\n",
    "    good_bas = []\n",
    "    for ba in bas:\n",
    "        if ba in usable_BAs:\n",
    "            good_bas.append(ba)\n",
    "    \n",
    "    \n",
    "    first_ba = good_bas.pop()\n",
    "    master = pd.read_csv(f'{out_base}/balancing_authorities/{first_ba}.csv', na_values=['MISSING', 'EMPTY'])\n",
    "    master = master.fillna(0)\n",
    "    \n",
    "    master = master.drop(['category', 'forecast demand (MW)'], axis=1)\n",
    "    \n",
    "    for ba in good_bas:\n",
    "        df = pd.read_csv(f'{out_base}/balancing_authorities/{ba}.csv', na_values=['MISSING', 'EMPTY'])\n",
    "        df = df.fillna(0)\n",
    "        master['raw demand (MW)'] += df['raw demand (MW)']\n",
    "        master['cleaned demand (MW)'] += df['cleaned demand (MW)']\n",
    "    \n",
    "    master.to_csv(f'{out_base}/{folder}/{region}.csv', index=False)\n",
    "    \n",
    "\n",
    "# Do both the distribution of balancing authority level results to new BA files\n",
    "# and generate regional, interconnect, and CONUS aggregate files.\n",
    "def distribute_MICE_results(raw_demand_file_loc, screening_file, mice_results_csv, out_base):\n",
    "\n",
    "    # Load screening results\n",
    "    screening = pd.read_csv(screening_file)\n",
    "    # Load MICE results\n",
    "    mice = pd.read_csv(mice_results_csv)\n",
    "    screening = trim_rows_to_match_length(mice, screening)\n",
    "    \n",
    "    # Distribute to single BA results files first\n",
    "    print(\"Distribute MICE results per-balancing authority:\")\n",
    "    for ba in return_usable_BAs():\n",
    "        print(ba)\n",
    "        df = pd.read_csv(f\"{raw_demand_file_loc}/{ba}.csv\")\n",
    "        df = trim_rows_to_match_length(mice, df)\n",
    "    \n",
    "        df_out = pd.DataFrame({\n",
    "            'date_time': df['date_time'],\n",
    "            'raw demand (MW)': df['demand (MW)'],\n",
    "            'category': screening[f'{ba}_category'],\n",
    "            'cleaned demand (MW)': mice[ba],\n",
    "            'forecast demand (MW)': df['forecast demand (MW)']\n",
    "        })\n",
    "        \n",
    "        \n",
    "        df_out.to_csv(f'./{out_base}/balancing_authorities/{ba}.csv', index=False)\n",
    "\n",
    "    # Aggregate balancing authority level results into EIA regions\n",
    "    print(\"\\nEIA regional aggregation:\")\n",
    "    for region, bas in return_BAs_per_region_map().items():\n",
    "        merge_BAs(region, bas, out_base, 'regions')\n",
    "    \n",
    "    # Aggregate balancing authority level results into CONUS interconnects\n",
    "    print(\"\\nCONUS interconnect aggregation:\")\n",
    "    for region, bas in return_ICs_from_BAs().items():\n",
    "        merge_BAs(region, bas, out_base, 'interconnects')\n",
    "    \n",
    "    # Aggregate balancing authority level results into CONUS total\n",
    "    print(\"\\nCONUS total aggregation:\")\n",
    "    merge_BAs('CONUS', return_usable_BAs(), out_base, 'contiguous_US')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the distribution and aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output file generated by Step 2 listing the categories for each time step\n",
    "screening_file = './data/csv_MASTER.csv'\n",
    "# The output file generated by Step 3 which runs the MICE algo and has the cleaned demand values\n",
    "mice_file = 'MICE_output/mean_impute_csv_MASTER.csv'\n",
    "\n",
    "\n",
    "distribute_MICE_results(input_path, screening_file, mice_file, out_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test distribution and aggregation\n",
    "This cell simply checks that the results all add up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare each value in the vectors\n",
    "def compare(vect1, vect2):\n",
    "    cnt = 0\n",
    "    clean = True\n",
    "    for v1, v2 in zip(vect1, vect2):\n",
    "        if v1 != v2:\n",
    "            print(f\"Error at idx {cnt} {v1} != {v2}\")\n",
    "            clean = False\n",
    "        cnt += 1\n",
    "    return clean\n",
    "    \n",
    "\n",
    "def test_aggregation(raw_demand_file_loc, screening_file, mice_results_csv, out_base):\n",
    "\n",
    "    # Load MICE results\n",
    "    usable_BAs = return_usable_BAs()\n",
    "    mice = pd.read_csv(mice_results_csv)\n",
    "\n",
    "    # Sum all result BAs\n",
    "    tot_imp = np.zeros(len(mice.index))\n",
    "    for col in mice.columns:\n",
    "        if col not in usable_BAs:\n",
    "            continue\n",
    "        tot_imp += mice[col]\n",
    "\n",
    "    # Sum Raw\n",
    "    tot_raw = np.zeros(len(mice.index))\n",
    "    for ba in return_usable_BAs():\n",
    "        df = pd.read_csv(f\"{raw_demand_file_loc}/{ba}.csv\", na_values=['MISSING', 'EMPTY'])\n",
    "        df = trim_rows_to_match_length(mice, df)\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        tot_raw += df['demand (MW)']\n",
    "    \n",
    "    # Check BA results distribution\n",
    "    print(\"\\nBA Distribution:\")\n",
    "    new_tot_raw = np.zeros(len(mice.index))\n",
    "    new_tot_clean = np.zeros(len(mice.index))\n",
    "    for ba in return_usable_BAs():\n",
    "        df = pd.read_csv(f\"{out_base}/balancing_authorities/{ba}.csv\", na_values=['MISSING', 'EMPTY'])\n",
    "        df = df.fillna(0)\n",
    "        new_tot_raw += df['raw demand (MW)']\n",
    "        new_tot_clean += df['cleaned demand (MW)']\n",
    "    \n",
    "    assert(compare(tot_raw, new_tot_raw)), \"Error in raw sums.\"\n",
    "    assert(compare(tot_imp, new_tot_clean)), \"Error in imputed values.\"\n",
    "    print(\"BA Distribution okay!\")\n",
    "    \n",
    "    \n",
    "    # Check aggregate balancing authority level results into EIA regions\n",
    "    print(\"\\nEIA regional aggregation:\")\n",
    "    new_tot_raw = np.zeros(len(mice.index))\n",
    "    new_tot_clean = np.zeros(len(mice.index))\n",
    "    for region, bas in return_BAs_per_region_map().items():\n",
    "        df = pd.read_csv(f\"{out_base}/regions/{region}.csv\")\n",
    "        new_tot_raw += df['raw demand (MW)']\n",
    "        new_tot_clean += df['cleaned demand (MW)']\n",
    "    \n",
    "    assert(compare(tot_raw, new_tot_raw)), \"Error in raw sums.\"\n",
    "    assert(compare(tot_imp, new_tot_clean)), \"Error in imputed values.\"\n",
    "    print(\"Regional sums okay!\")\n",
    "    \n",
    "    # Aggregate balancing authority level results into CONUS interconnects\n",
    "    print(\"\\nCONUS interconnect aggregation:\")\n",
    "    new_tot_raw = np.zeros(len(mice.index))\n",
    "    new_tot_clean = np.zeros(len(mice.index))\n",
    "    for region, bas in return_ICs_from_BAs().items():\n",
    "        df = pd.read_csv(f\"{out_base}/interconnects/{region}.csv\")\n",
    "        new_tot_raw += df['raw demand (MW)']\n",
    "        new_tot_clean += df['cleaned demand (MW)']\n",
    "    \n",
    "    assert(compare(tot_raw, new_tot_raw)), \"Error in raw sums.\"\n",
    "    assert(compare(tot_imp, new_tot_clean)), \"Error in imputed values.\"\n",
    "    print(\"Interconnect sums okay!\")\n",
    "           \n",
    "           \n",
    "    # Aggregate balancing authority level results into CONUS total\n",
    "    print(\"\\nCONUS total aggregation:\")\n",
    "    new_tot_raw = np.zeros(len(mice.index))\n",
    "    new_tot_clean = np.zeros(len(mice.index))\n",
    "    df = pd.read_csv(f\"{out_base}/contiguous_US/CONUS.csv\")\n",
    "    new_tot_raw += df['raw demand (MW)']\n",
    "    new_tot_clean += df['cleaned demand (MW)']\n",
    "    \n",
    "    assert(compare(tot_raw, new_tot_raw)), \"Error in raw sums.\"\n",
    "    assert(compare(tot_imp, new_tot_clean)), \"Error in imputed values.\"\n",
    "    print(\"CONUS sums okay!\")\n",
    "\n",
    "\n",
    "test_aggregation(input_path, screening_file, mice_file, out_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
